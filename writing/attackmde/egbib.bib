@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{Sharif_2017_CCS,
author = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
title = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978392},
doi = {10.1145/2976749.2978392},
abstract = {Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk.In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1528â€“1540},
numpages = {13},
keywords = {adversarial machine learning, neural networks, face recognition, face detection},
location = {Vienna, Austria},
series = {CCS '16}
}

@InProceedings{Zolfi_2021_CVPR,
    author    = {Zolfi, Alon and Kravchik, Moshe and Elovici, Yuval and Shabtai, Asaf},
    title     = {The Translucent Patch: A Physical and Universal Attack on Object Detectors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15232-15241}
}

@inproceedings{Wong_2020_NIPS,
 author = {Wong, Alex and Cicek, Safa and Soatto, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {8486--8497},
 publisher = {Curran Associates, Inc.},
 title = {Targeted Adversarial Perturbations for Monocular Depth Prediction},
 url = {https://proceedings.neurips.cc/paper/2020/file/609e9d4bcc8157c00808993f612f1acd-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Liu_2019_AAAI,
author = {Liu, Aishan and Liu, Xianglong and Fan, Jiaxin and Ma, Yuqing and Zhang, Anlan and Xie, Huiyuan and Tao, Dacheng},
title = {Perceptual-Sensitive GAN for Generating Adversarial Patches},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33011028},
doi = {10.1609/aaai.v33i01.33011028},
abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Recently, adversarial patch, with noise confined to a small and localized patch, emerged for its easy accessibility in real-world. However, existing attack strategies are still far from generating visually natural patches with strong attacking ability, since they often ignore the perceptual sensitivity of the attacked network to the adversarial patch, including both the correlations with the image context and the visual attention. To address this problem, this paper proposes a perceptual-sensitive generative adversarial network (PS-GAN) that can simultaneously enhance the visual fidelity and the attacking ability for the adversarial patch. To improve the visual fidelity, we treat the patch generation as a patch-to-patch translation via an adversarial process, feeding any types of seed patch and outputting the similar adversarial patch with high perceptual correlation with the attacked image. To further enhance the attacking ability, an attention mechanism coupled with adversarial generation is introduced to predict the critical attacking areas for placing the patches, which can help producing more realistic and aggressive patches. Extensive experiments under semi-whitebox and black-box settings on two large-scale datasets GTSRB and ImageNet demonstrate that the proposed PS-GAN outperforms state-of-the-art adversarial patch attack methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {127},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}


@InProceedings{Hu_2021_ICCV,
    author    = {Hu, Yu-Chih-Tuan and Kung, Bo-Han and Tan, Daniel Stanley and Chen, Jun-Cheng and Hua, Kai-Lung and Cheng, Wen-Huang},
    title     = {Naturalistic Physical Adversarial Patch for Object Detectors},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {7848-7857}
}

@InProceedings{Kong_2020_CVPR,
author = {Kong, Zelun and Guo, Junfeng and Li, Ang and Liu, Cong},
title = {PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{Eykholt_2018_CVPR,
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
title = {Robust Physical-World Attacks on Deep Learning Visual Classification},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@inproceedings{Szegedy_2014_ICLR,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Intriguing properties of neural networks},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6199},
  timestamp = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Brown_2017_arxiv,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv preprint arXiv:1712.09665},
  year={2017}
}

@inproceedings{Eigen_2014_nips,
 author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
 url = {https://proceedings.neurips.cc/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
 volume = {27},
 year = {2014}
}

@InProceedings{Eigen_2015_ICCV,
author = {Eigen, David and Fergus, Rob},
title = {Predicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}
@article{lee_2019_arxiv,
  title={From big to small: Multi-scale local planar guidance for monocular depth estimation},
  author={Lee, Jin Han and Han, Myung-Kyu and Ko, Dong Wook and Suh, Il Hong},
  journal={arXiv preprint arXiv:1907.10326},
  year={2019}
}
@INPROCEEDINGS{Wofk_2019_ICRA,
  author={Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={FastDepth: Fast Monocular Depth Estimation on Embedded Systems}, 
  year={2019},
  volume={},
  number={},
  pages={6101-6108},
  doi={10.1109/ICRA.2019.8794182}}
@inproceedings{Fu_2018_CVPR,
  AUTHOR = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  TITLE = {{Deep Ordinal Regression Network for Monocular Depth Estimation}},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  YEAR = {2018}
}
@article{cao_2017_CSVT,
  title={Estimating depth from monocular images as classification using deep fully convolutional residual networks},
  author={Cao, Yuanzhouhan and Wu, Zifeng and Shen, Chunhua},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={28},
  number={11},
  pages={3174--3182},
  year={2017},
  publisher={IEEE}
}
@InProceedings{Li_2018_ACCV,
author="Li, Ruibo
and Xian, Ke
and Shen, Chunhua
and Cao, Zhiguo
and Lu, Hao
and Hang, Lingxiao",
editor="Jawahar, C.V.
and Li, Hongdong
and Mori, Greg
and Schindler, Konrad",
title="Deep Attention-Based Classification Network for Robust Depth Prediction",
booktitle="Computer Vision -- ACCV 2018",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="663--678",
abstract="In this paper, we present our deep attention-based classification (DABC) network for robust single image depth prediction, in the context of the Robust Vision Challenge 2018 (ROB 2018) (http://www.robustvision.net/index.php). Unlike conventional depth prediction, our goal is to design a model that can perform well in both indoor and outdoor scenes with a single parameter set. However, robust depth prediction suffers from two challenging problems: (a) How to extract more discriminative features for different scenes (compared to a single scene)? (b) How to handle the large differences of depth ranges between indoor and outdoor datasets? To address these two problems, we first formulate depth prediction as a multi-class classification task and apply a softmax classifier to classify the depth label of each pixel. We then introduce a global pooling layer and a channel-wise attention mechanism to adaptively select the discriminative channels of features and to update the original features by assigning important channels with higher weights. Further, to reduce the influence of quantization errors, we employ a soft-weighted sum inference strategy for the final prediction. Experimental results on both indoor and outdoor datasets demonstrate the effectiveness of our method. It is worth mentioning that we won the 2-nd place in single image depth prediction entry of ROB 2018, in conjunction with IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018.",
isbn="978-3-030-20870-7"
}
@inproceedings{Lee_2019_CVPR,
  title={Monocular depth estimation using relative depth maps},
  author={Lee, Jae-Han and Kim, Chang-Su},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2019}
}

@inproceedings{Godard_2017_CVPR,
  title={Unsupervised monocular depth estimation with left-right consistency},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={270--279},
  year={2017}
}

@INPROCEEDINGS{Zhou_2017_CVPR,  
author={T. {Zhou} and M. {Brown} and N. {Snavely} and D. G. {Lowe}},  
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
title={Unsupervised Learning of Depth and Ego-Motion from Video},   
year={2017},  
volume={},  
number={},  
pages={6612-6619},  
doi={10.1109/CVPR.2017.700}}

@InProceedings{Godard_2019_ICCV,
author = {Godard, Clement and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel J.},
title = {Digging Into Self-Supervised Monocular Depth Estimation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
} 
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{tanaka2018joint,
  title={Joint optimization framework for learning with noisy labels},
  author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5552--5560},
  year={2018}
}
@inproceedings{yao2019safeguarded,
  title={Safeguarded dynamic label regression for noisy supervision},
  author={Yao, Jiangchao and Wu, Hao and Zhang, Ya and Tsang, Ivor W and Sun, Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={9103--9110},
  year={2019}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrzebski, Stanislaw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International Conference on Machine Learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}
@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}

@inproceedings{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert R},
  booktitle={32nd Conference on Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@inproceedings{yao2021jo,
  title={Jo-SRC: A Contrastive Approach for Combating Noisy Labels},
  author={Yao, Yazhou and Sun, Zeren and Zhang, Chuanyi and Shen, Fumin and Wu, Qi and Zhang, Jian and Tang, Zhenmin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5192--5201},
  year={2021}
}

@article{li2020dividemix,
  title={Dividemix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2002.07394},
  year={2020}
}

@inproceedings{kim2019nlnl,
  title={Nlnl: Negative learning for noisy labels},
  author={Kim, Youngdong and Yim, Junho and Yun, Juseung and Kim, Junmo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={101--110},
  year={2019}
}

@inproceedings{yu2019does,
  title={How does disagreement help generalization against label corruption?},
  author={Yu, Xingrui and Han, Bo and Yao, Jiangchao and Niu, Gang and Tsang, Ivor and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={7164--7173},
  year={2019},
  organization={PMLR}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  pages={896},
  year={2013}
}

@article{han2018co,
  title={Co-teaching: Robust training of deep neural networks with extremely noisy labels},
  author={Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1804.06872},
  year={2018}
}

@article{rizve2021defense,
  title={In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning},
  author={Rizve, Mamshad Nayeem and Duarte, Kevin and Rawat, Yogesh S and Shah, Mubarak},
  journal={arXiv preprint arXiv:2101.06329},
  year={2021}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{ishida2017learning,
  title={Learning from complementary labels},
  author={Ishida, Takashi and Niu, Gang and Hu, Weihua and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1705.07541},
  year={2017}
}

@inproceedings{ishida2019complementary,
  title={Complementary-label learning for arbitrary losses and models},
  author={Ishida, Takashi and Niu, Gang and Menon, Aditya and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={2971--2980},
  year={2019},
  organization={PMLR}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{stn,
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
title = {Spatial Transformer Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2017â€“2025},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}
@inproceedings{Silberman_2012_ECCV,
  author    = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title     = {Indoor Segmentation and Support Inference from RGBD Images},
  booktitle = {ECCV},
  year      = {2012}
}
@misc{quickdraw,
  author          = {J. Jongejan, H. Rowley, T. K. J. K. and Fox-Gieg.},
  booktitle       = {a.i. experiment.},
  title           = {The quick, draw!},
  editor          = {},
  year            = {2016},
  note            = {\url{https://github.com/googlecreativelab/quickdraw-dataset}}
}

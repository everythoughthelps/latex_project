@InProceedings{Godard_2019_ICCV,
author = {Godard, Clement and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel J.},
title = {Digging Into Self-Supervised Monocular Depth Estimation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
} 
@inproceedings{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in neural information processing systems},
  pages={8026--8037},
  year={2019}
}

@inproceedings{FuCVPR18-DORN,
  AUTHOR = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  TITLE = {{Deep Ordinal Regression Network for Monocular Depth Estimation}},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  YEAR = {2018}
}

@inproceedings{laina2016deeper,
        author={Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
        title={Deeper depth prediction with fully convolutional residual networks},
        booktitle={3D Vision (3DV), 2016 Fourth International Conference on},
        pages={239--248},
        year={2016},
        organization={IEEE}
}

@INPROCEEDINGS{zhou2017unsupervised,  author={T. {Zhou} and M. {Brown} and N. {Snavely} and D. G. {Lowe}},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Unsupervised Learning of Depth and Ego-Motion from Video},   year={2017},  volume={},  number={},  pages={6612-6619},  doi={10.1109/CVPR.2017.700}}

@inproceedings{saxena2006learning,
  author={Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
  title={Learning depth from single monocular images},
  booktitle={Advances in neural information processing systems},
  pages={1161--1168},
  year={2006}
}

@inproceedings{eigen2014depth,
  author={Eigen, David and Puhrsch, Christian and Fergus, Rob},
  title={Depth map prediction from a single image using a multi-scale deep network},
  booktitle={Advances in neural information processing systems},
  pages={2366--2374},
  year={2014}
}


@inproceedings{zou2010method,
  title={A method of stereo vision matching based on OpenCV},
  author={Zou, Ling and Li, Yan},
  booktitle={2010 International Conference on Audio, Language and Image Processing},
  pages={185--190},
  year={2010},
  organization={IEEE}
}
@article{cao2015summary,
  title={Summary of binocular stereo vision matching technology},
  author={Cao, Zhi-Le and Yan, Zhong-Hong and Wang, Hong},
  journal={Journal of Chongqing University of Technology (Natural Science)},
  volume={29},
  number={2},
  pages={70--75},
  year={2015}
}

@article{ullman1979interpretation,
  title={The interpretation of structure from motion},
  author={Ullman, Shimon},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={203},
  number={1153},
  pages={405--426},
  year={1979},
  publisher={The Royal Society London}
}

@article{geiger2013vision,
  title={Vision meets robotics: The kitti dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{Silberman:ECCV12,
  author    = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title     = {Indoor Segmentation and Support Inference from RGBD Images},
  booktitle = {ECCV},
  year      = {2012}
}

@inproceedings{yoneda2014lidar,
  title={Lidar scan feature for localization with highly precise 3-D map},
  author={Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
  booktitle={2014 IEEE Intelligent Vehicles Symposium Proceedings},
  pages={1345--1350},
  year={2014},
  organization={IEEE}
}

@article{zhang2012microsoft,
  title={Microsoft kinect sensor and its effect},
  author={Zhang, Zhengyou},
  journal={IEEE multimedia},
  volume={19},
  number={2},
  pages={4--10},
  year={2012},
  publisher={IEEE}
}
@article{10.1145/3065386,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}
@INPROCEEDINGS{7410661,
  author={D. {Eigen} and R. {Fergus}},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture}, 
  year={2015},
  volume={},
  number={},
  pages={2650-2658},
  doi={10.1109/ICCV.2015.304}}

@inproceedings{liu2015deep,
  title={Deep convolutional neural fields for depth estimation from a single image},
  author={Liu, Fayao and Shen, Chunhua and Lin, Guosheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5162--5170},
  year={2015}
}

@article{liu2015learning,
  title={Learning depth from single monocular images using deep convolutional neural fields},
  author={Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={2024--2039},
  year={2015},
  publisher={IEEE}
}

@inproceedings{li2015depth,
  title={Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs},
  author={Li, Bo and Shen, Chunhua and Dai, Yuchao and Van Den Hengel, Anton and He, Mingyi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1119--1127},
  year={2015}
}
@article{cao2017estimating,
  title={Estimating depth from monocular images as classification using deep fully convolutional residual networks},
  author={Cao, Yuanzhouhan and Wu, Zifeng and Shen, Chunhua},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={28},
  number={11},
  pages={3174--3182},
  year={2017},
  publisher={IEEE}
}
@inproceedings{garg2016unsupervised,
  title={Unsupervised cnn for single view depth estimation: Geometry to the rescue},
  author={Garg, Ravi and Bg, Vijay Kumar and Carneiro, Gustavo and Reid, Ian},
  booktitle={European conference on computer vision},
  pages={740--756},
  year={2016},
  organization={Springer}
}
@inproceedings{godard2017unsupervised,
  title={Unsupervised monocular depth estimation with left-right consistency},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={270--279},
  year={2017}
}

@INPROCEEDINGS{Kuznietsov2017,  author={Y. {Kuznietsov} and J. {Stückler} and B. {Leibe}},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Semi-Supervised Deep Learning for Monocular Depth Map Prediction},   year={2017},  volume={},  number={},  pages={2215-2223},  doi={10.1109/CVPR.2017.238}}

@inproceedings{zoran2015learning,
  title={Learning ordinal relationships for mid-level vision},
  author={Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={388--396},
  year={2015}
}
@inproceedings{lee2019monocular,
  title={Monocular depth estimation using relative depth maps},
  author={Lee, Jae-Han and Kim, Chang-Su},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2019}
}

@inproceedings{NIPS2016_0deb1c54,
 author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {730--738},
 publisher = {Curran Associates, Inc.},
 title = {Single-Image Depth Perception in the Wild},
 volume = {29},
 year = {2016}
}

@INPROCEEDINGS{2016semantic,
  author={A. {Mousavian} and H. {Pirsiavash} and J. {Košecká}},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
  title={Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks}, 
  year={2016},
  volume={},
  number={},
  pages={611-619},
  doi={10.1109/3DV.2016.69}}

@INPROCEEDINGS{2015semantic,
author={ {Peng Wang} and  {Xiaohui Shen} and  {Zhe Lin} and S. {Cohen} and B. {Price} and A. {Yuille}},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Towards unified depth and semantic prediction from a single image}, 
year={2015},
volume={},
number={},
pages={2800-2809},
doi={10.1109/CVPR.2015.7298897}}

@INPROCEEDINGS{2019semantic,  author={P. {Chen} and A. H. {Liu} and Y. {Liu} and Y. F. {Wang}},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Towards Scene Understanding: Unsupervised Monocular Depth Estimation With Semantic-Aware Representation},   year={2019},  volume={},  number={},  pages={2619-2627},  doi={10.1109/CVPR.2019.00273}}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}
@article{anil2018large,
  title={Large scale distributed neural network training through online distillation},
  author={Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1804.03235},
  year={2018}
}
@INPROCEEDINGS{zhang_self_distillation,  author={L. {Zhang} and J. {Song} and A. {Gao} and J. {Chen} and C. {Bao} and K. {Ma}},  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},   title={Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},   year={2019},  volume={},  number={},  pages={3712-3721},  doi={10.1109/ICCV.2019.00381}}

@article{bts,
  author    = {Jin Han Lee and
               Myung{-}Kyu Han and
               Dong Wook Ko and
               Il Hong Suh},
  title     = {From Big to Small: Multi-Scale Local Planar Guidance for Monocular
               Depth Estimation},
  journal   = {CoRR},
  volume    = {abs/1907.10326},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1907.10326},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{DABC,
author="Li, Ruibo
and Xian, Ke
and Shen, Chunhua
and Cao, Zhiguo
and Lu, Hao
and Hang, Lingxiao",
editor="Jawahar, C.V.
and Li, Hongdong
and Mori, Greg
and Schindler, Konrad",
title="Deep Attention-Based Classification Network for Robust Depth Prediction",
booktitle="Computer Vision -- ACCV 2018",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="663--678",
abstract="In this paper, we present our deep attention-based classification (DABC) network for robust single image depth prediction, in the context of the Robust Vision Challenge 2018 (ROB 2018) (http://www.robustvision.net/index.php). Unlike conventional depth prediction, our goal is to design a model that can perform well in both indoor and outdoor scenes with a single parameter set. However, robust depth prediction suffers from two challenging problems: (a) How to extract more discriminative features for different scenes (compared to a single scene)? (b) How to handle the large differences of depth ranges between indoor and outdoor datasets? To address these two problems, we first formulate depth prediction as a multi-class classification task and apply a softmax classifier to classify the depth label of each pixel. We then introduce a global pooling layer and a channel-wise attention mechanism to adaptively select the discriminative channels of features and to update the original features by assigning important channels with higher weights. Further, to reduce the influence of quantization errors, we employ a soft-weighted sum inference strategy for the final prediction. Experimental results on both indoor and outdoor datasets demonstrate the effectiveness of our method. It is worth mentioning that we won the 2-nd place in single image depth prediction entry of ROB 2018, in conjunction with IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018.",
isbn="978-3-030-20870-7"
}
@ARTICLE{Make3D,
  author={A. {Saxena} and M. {Sun} and A. Y. {Ng}}, 
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title={Make3D: Learning 3D Scene Structure from a Single Still Image},
   year={2009}, 
    volume={31},  
    number={5},
     pages={824-840}, 
     doi={10.1109/TPAMI.2008.132}}

@misc{nyu-toolbox,
  author = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title = {Indoor Segmentation and Support Inference from RGBD Images},
note = {\url{https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html}}
}

@inproceedings{xu2018structured,
  title={Structured attention guided convolutional neural fields for monocular depth estimation},
  author={Xu, Dan and Wang, Wei and Tang, Hao and Liu, Hong and Sebe, Nicu and Ricci, Elisa},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3917--3925},
  year={2018}
}
@article{chakrabarti2016depth,
  title={Depth from a single image by harmonizing overcomplete local network predictions},
  author={Chakrabarti, Ayan and Shao, Jingyu and Shakhnarovich, Gregory},
  journal={arXiv preprint arXiv:1605.07081},
  year={2016}
}

@INPROCEEDINGS{frustumpointnet,  
author={C. R. {Qi} and W. {Liu} and C. {Wu} and H. {Su} and L. J. {Guibas}},  
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},   
title={Frustum PointNets for 3D Object Detection from RGB-D Data},   
year={2018},  
volume={},  
number={},  
pages={918-927},  
doi={10.1109/CVPR.2018.00102}
}

@ARTICLE{RT3D,  author={Y. {Zeng} and Y. {Hu} and S. {Liu} and J. {Ye} and Y. {Han} and X. {Li} and N. {Sun}},  journal={IEEE Robotics and Automation Letters},   title={RT3D: Real-Time 3-D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving},   year={2018},  volume={3},  number={4},  pages={3434-3440},  doi={10.1109/LRA.2018.2852843}}

@INPROCEEDINGS{PointRCNN,  author={S. {Shi} and X. {Wang} and H. {Li}},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud},   year={2019},  volume={},  number={},  pages={770-779},  doi={10.1109/CVPR.2019.00086}}
@article{dai2017bundlefusion,
  title={BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration},
  author={Dai, Angela and Nie{\ss}ner, Matthias and Zoll{\"o}fer, Michael and Izadi, Shahram and Theobalt, Christian},
  journal={ACM Transactions on Graphics 2017 (TOG)},
  year={2017}
}
@article{ElasticFusion,
author = {Whelan, Thomas and Salas-Moreno, Renato F and Glocker, Ben and Davison, Andrew J and Leutenegger, Stefan},
title = {ElasticFusion},
year = {2016},
issue_date = {12 2016},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {35},
number = {14},
issn = {0278-3649},
doi = {10.1177/0278364916669237},
abstract = {We present a novel approach to real-time dense visual simultaneous localisation and mapping. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution, while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables a more realistic augmented reality rendering; b a richer understanding of the scene beyond pure geometry and; c more accurate and robust photometric tracking.},
journal = {Int. J. Rob. Res.},
month = dec,
pages = {1697–1716},
numpages = {20},
keywords = {RGB-D, dense methods, Surfel fusion, large scale, reflections, specular, camera pose estimation, SLAM, real-time, GPU, light sources}
}
@inproceedings{mrslasermap,
abstract = {In this paper, we present a three-dimensional mapping system for mobile robots using laser range sensors. Our system provides sensor preprocessing, efficient local mapping for reliable obstacle perception, and allocentric mapping with real-time localization for autonomous navigation. The software is available as open-source ROS-based package and has been successfully employed on different robotic platforms, such as micro aerial vehicles and ground robots in different research projects and robot competitions. Core of our approach are local multiresolution grid maps and an efficient surfel-based registration method to aggregate measurements from consecutive laser scans. By using local multiresolution grid maps as central data structure in our system, we gain computational efficiency by having high resolution in the near vicinity of the robot and lower resolution with increasing distance. Furthermore, local multiresolution grid maps provide a probabilistic representation of the environment---allowing us to address dynamic objects and to distinguish between occupied, free, and unknown areas. Spatial relations between local maps are modeled in a graph-based structure, enabling allocentric mapping and localization.},
address = {Cham},
author = {Droeschel, David and Behnke, Sven},
booktitle = {RoboCup 2016: Robot World Cup XX},
editor = {Behnke, Sven and Sheh, Raymond and Sar$\backslash$iel, Sanem and Lee, Daniel D},
isbn = {978-3-319-68792-6},
pages = {319--326},
publisher = {Springer International Publishing},
title = {MRSLaserMap: Local Multiresolution Grids for Efficient 3D Laser Mapping and Localization},
year = {2017}
}

@INPROCEEDINGS{RoutedFusion,
  author={S. {Weder} and J. {Schönberger} and M. {Pollefeys} and M. R. {Oswald}},  
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   
  title={RoutedFusion: Learning Real-Time Depth Map Fusion},   
  year={2020},  
  volume={},  
  number={},  
  pages={4886-4896},  
  doi={10.1109/CVPR42600.2020.00494}
  }
